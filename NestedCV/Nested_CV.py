import os
import shutil
import time
import signal
import pickle
import numpy as np
# Cross-validation and Grid Search
from sklearn.model_selection import KFold, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
# Models
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis
# Custom models
from skelm import ELMClassifier
from CustomModels import EnsembleDeepRVFL, RVC, BroadNet, SCN, SupervisedOPF
from skrules import SkopeRules
from gplearn.genetic import SymbolicClassifier
from lightgbm.sklearn import LGBMClassifier
from catboost.catboost import CatBoostClassifier
from xgboost.sklearn import XGBClassifier
# Metrics
from sklearn.utils import shuffle
from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, recall_score, precision_score, matthews_corrcoef, cohen_kappa_score
# Dataset
#import ember


# Global variables

# Cross-Validation parameters
outer_splits = 5
inner_splits = 3
n_jobs = 8

# Model name list
model_names = [
        'K-Nearest Neighbors',
        'Support Vector Machine',
        'Decision Tree',
        'Random Forest',
        'Extreme Random Forest',
        'Gradient Boosted Machine',
        'MultiLayer Perceptron',
        'AdaBoost',
        'Gaussian Naive Bayes',
        'Extreme Learning Machine',
        'Ensemble Deep RVFL',
        'LightGBM',
        'Skope Rules',
        'GPLearn',
        'Relevance Vector Machine',
        'CatBoost',
        'XGBoost',
        'Broad Learning System',
        'Stochastic Configuration Network',
        'Logistic Regressor',
        'Linear Discriminant Analysis',
        'Quadratic Discriminant Analysis',
        'Optimus Path Forest'
        ]

# Model acronym list
model_acronyms = [
        'knn',
        'svc',
        'dt',
        'rf',
        'erf',
        'gbm',
        'mlp',
        'ab',
        'gnb',
        'elm',
        'edrvfl',
        'lgbm',
        'sr',
        'gpl',
        'rvm',
        'cb',
        'xgb',
        'bls',
        'scn',
        'lr',
        'lda',
        'qda',
        'opf'
        ]

# Model list
models = [
        KNeighborsClassifier(), 
        SVC(),
        DecisionTreeClassifier(), 
        RandomForestClassifier(), 
        ExtraTreesClassifier(),
        GradientBoostingClassifier(),
        MLPClassifier(),
        AdaBoostClassifier(),
        GaussianNB(),
        ELMClassifier(),
        EnsembleDeepRVFL(),
        LGBMClassifier(),
        SkopeRules(),
        SymbolicClassifier(),
        RVC(),
        CatBoostClassifier(),
        XGBClassifier(),
        BroadNet(),
        SCN(),
        LogisticRegression(),
        LinearDiscriminantAnalysis(),
        QuadraticDiscriminantAnalysis(),
        SupervisedOPF()
        ]

# Models parameter grid
models_param_grid = [
    {
        # K-Nearest Neighbors
        'knn__n_neighbors' : [1, 3, 5, 7, 9],
        'knn__weights' : ["uniform", "distance"],
        'knn__p' : [1, 2, 3]
    },
    {
        # Support Vector Machine
        'svc__C' : [0.1, 1, 10, 100, 1000],
        'svc__gamma': [0.0001, 0.001, 0.01, 0.1, 1]
    },
    {
        # Decision tree
        'dt__criterion' : ["gini", "entropy"],
        'dt__splitter' : ["best", "random"],
        'dt__max_depth' : [None, 10, 20, 50]
    },
    {
        # Random Forest
        'rf__n_estimators' : [50, 100, 200],
        'rf__criterion' : ["gini", "entropy"],
        'rf__max_depth' : [None, 10, 20, 50]
    },
    {
        # Extreme Random Forest
        'erf__n_estimators' : [50, 100, 200],
        'erf__criterion' : ["gini", "entropy"],
        'erf__max_depth' : [None, 10, 20, 50]
    },
    {
        # Gradient Boosted Machine
        'gbm__loss' : ["deviance", 'exponential'],
        'gbm__learning_rate' : [0.01, 0.1, 1],
        'gbm__n_estimators' : [50, 100, 200], 
    },
    {
        # MultiLayer Perceptron
        'mlp__hidden_layer_sizes' : [(1000, 500, 200), (500, 200, 100), (200, 100, 50)],
        'mlp__activation' : ['identity', 'logistic', 'relu'],
        'mlp__alpha' : [0.00001, 0.0001, 0.001],
        'mlp__learning_rate_init' : [0.001, 0.01, 0.1]
    },
    {
        # Adaboost
        'ab__n_estimators' : [200, 500, 1000], 
        'ab__learning_rate' : [0.01, 0.1, 1]
    
    },
    {
        # Gaussian Naive Bayes
        'gnb__var_smoothing' : np.logspace(0, -9, 50)
    },
    {
        # Extreme Learning Machine
        'elm__n_neurons' : [200, 500, 1000],
        'elm__alpha' : [0.0001, 0.001, 0.01, 0.1],
    },
    {
        # Ensemble Deep RVFL
        'edrvfl__n_nodes' : [40, 80, 100, 200, 300],
        'edrvfl__n_layer' : [1, 2, 3, 4, 5],
        "edrvfl__activation" : ["relu", "sigmoid"]
    },
    {
        # LightGBM
        'lgbm__num_leaves' : [50, 500, 1000],
        'lgbm__max_depth' : [3, 6, 12]
    },
    {
        # Skope Rules
        'sr__max_depth_duplication' : [None, 2, 3],
        'sr__n_estimators' : [200, 500, 1000]
    },
    {
        # GPLearn
        'gpl__population_size' : [100, 1000, 10000],
        'gpl__generations' : [20, 50, 200],
        'gpl__tournament_size' : [20, 50, 200]
    },
    {
        # RVM
        'rvm__kernel' : ['linear', 'rbf'],
        'rvm__alpha' : [1e-6, 1e-5, 1e-4],
        'rvm__bias_used' : [False]
    },
    {
        # CatBoost
        'cb__l2_leaf_reg' : [1, 3, 5],
        'cb__learning_rate' : [0.001, 0.01, 0.1],
        'cb__depth' : [4, 6, 10],
        'cb__silent' : [True]
    },
    {
        # XGBoost
        'xgb__n_estimators' : [50, 100, 200],
        'xgb__max_depth' : [2, 4, 6],
        'xgb__eval_metric' : ['error'],
        'xgb__use_label_encoder' : [False]
    },
    {
        # BLS
        'bls__C' : [1e-04, 1e-03, 1e-02, 1e-01],
        'bls__N1' : [50, 100, 200, 500]

    },
    {
        # SCN
        'scn__L_max' : [10, 100, 1000],
        'scn__T_max' : [10, 100, 1000]
    },
        {
        # Logistic Regressor
        'lr__penalty' : ['l2', 'l1'],
        'lr__C' : [0.0001, 0.001, 0.1, 1],
        'lr__max_iter' : [10000],
        'lr__solver' : ['saga']
    },
    {
        # Linear Discriminant Analysis
        'lda__solver' : ['svd']
    },
    {
        # Quadractic Discriminant Analysis
        'qda__reg_param' : [0.001, 0.01, 0.1, 0]
    },
    {
        # OPF
        'opf__distance' : ['log_squared_euclidean', 'log_euclidean']
    }
    ]


# Ctrl + C
def handler(sig, frame):
    print("\nSaliendo...\n")
    exit(1)
    
signal.signal(signal.SIGINT, handler)


# Bytes to multiples
def bytesto(bytes, to, bsize=1024): 
    a = {'k' : 1, 'm': 2, 'g' : 3, 't' : 4, 'p' : 5, 'e' : 6 }
    r = float(bytes)
    return bytes / (bsize ** a[to])


# Clean ember vectors for cross-validation
def cleanVectors(X_train, y_train, X_test, y_test):
    # Reshape train vector
    X_train = np.reshape(X_train,(-1, 2381))

    # Delete not labeled data
    unlabeled_index = np.argwhere(y_train==-1).flatten()
    X_train = np.delete(X_train,unlabeled_index,0)
    y_train = np.delete(y_train,unlabeled_index,0)

    # Join full dataset
    X = np.append(X_train,X_test,axis=0)
    y = np.append(y_train,y_test)

    # Shuffle dataset
    X_shuffled, y_shuffled = shuffle(X,y,random_state=0)

    # Return labels set as int (Avoid error in EDRVFL model)
    return X_shuffled, y_shuffled.astype(int)


def crossValidation(X, y):
    # Full result dictionary
    results = dict()
    
    # Nested Score dictionary
    nested_score = dict()
    
    # Models folder
    model_folder = './Models/'
    if not os.path.exists(model_folder):
        os.mkdir(model_folder)
    
    # Check results folder
    if not os.path.exists('./Results/'):
        os.mkdir('Results')
    
    # Result file check
    result_filename = './Results/Results.txt'
    if os.path.exists(result_filename):
        os.remove(result_filename)
    
    # All models Nested CV starting time
    startFull = time.time()
    
    # Iterate over models
    for i,model in enumerate(models, start=0):       
        
        # Model Score lists
        bacc = list()
        f1 = list()
        recall = list()
        precision = list()
        mcc = list()
        kappa = list()
        estimators = list()
        best_params = list()
        memory = list()
        fit_time = list()
        predict_time = list()
        fold_time = list()
        total_time = list()
        
        # Model Score dictionary
        model_nested_score = dict()
        model_nested_score['bacc'] = bacc
        model_nested_score['f1'] = f1
        model_nested_score['recall'] = recall
        model_nested_score['precision'] = precision
        model_nested_score['mcc'] = mcc
        model_nested_score['kappa'] = kappa
        model_nested_score['estimators'] = estimators
        model_nested_score['best_params'] = best_params
        model_nested_score['memory'] = memory
        model_nested_score['fit_time'] = fit_time
        model_nested_score['predict_time'] = predict_time
        model_nested_score['fold_time'] = fold_time
        model_nested_score['total_time'] = total_time
        
        # Model Nested CV starting time
        startModel = time.time()
        
        # Configure Outer Cross-Validation Procedure
        cv_outer = KFold(n_splits=outer_splits,shuffle=True,random_state=1)

        # Configure Inner Cross-Validation Procedure
        cv_inner = KFold(n_splits=inner_splits,shuffle=True,random_state=1)
        
        # Standar Scaler Pipeline for all the models
        estimator = Pipeline(steps=[('scaler', StandardScaler()),
                                    (model_acronyms[i], model)])
        
        # Nested Cross Validation
        for train_ix, test_ix in cv_outer.split(X):
            
            # Fold starting time
            fold_time = time.time()
            
            # Split current dataset
            X_train, X_test = X[train_ix, :], X[test_ix, :]
            y_train, y_test = y[train_ix], y[test_ix]
            
            # Define grid search
            gs = GridSearchCV(
                estimator=estimator,
                param_grid=models_param_grid[i],
                scoring='accuracy',
                cv=cv_inner,
                n_jobs=n_jobs,
                verbose=1)

            # Make grid search
            result = gs.fit(X_train, y_train)
            
            # Select best performing model
            best_model = result.best_estimator_
            
            # Measure fit time
            fit_time_start = time.time()
            best_model.fit(X_train, y_train)
            model_nested_score['fit_time'].append(time.time() - fit_time_start)
                        
            # Prediction
            predict_time_start = time.time()
            y_pred = best_model.predict(X_test)
            model_nested_score['predict_time'].append(time.time() - predict_time_start)
            
            # Score and store
            model_nested_score['bacc'].append(balanced_accuracy_score(y_test, y_pred))
            model_nested_score['f1'].append(f1_score(y_test, y_pred))
            model_nested_score['recall'].append(recall_score(y_test, y_pred))
            model_nested_score['precision'].append(precision_score(y_test, y_pred))
            model_nested_score['mcc'].append(matthews_corrcoef(y_test, y_pred))
            model_nested_score['kappa'].append(cohen_kappa_score(y_test, y_pred))
            model_nested_score['estimators'].append(best_model)
            model_nested_score['best_params'].append(result.best_params_)
            model_nested_score['fold_time'].append(time.time() - fold_time)
        
           
        # Model Nested CV ending time
        endModel = time.time()
                
        # Time of the best estimator in each fold
        for j,estimator in enumerate(model_nested_score['estimators'], start=1):
            
            # Save models one by one
            models_filename = model_folder + model_acronyms[i] + str(j) + '.pkl'
            with open(models_filename,'wb') as file:
                pickle.dump(estimator[model_acronyms[i]], file)
            
            # Memory size
            file_size = os.path.getsize(models_filename)
            model_nested_score['memory'].append(file_size)
            
            # Delete model
            os.remove(models_filename)
            
        
        # Save model full time
        model_nested_score['total_time'].append(endModel - startModel)
        
        # Save model results
        nested_score[model_acronyms[i]] = model_nested_score
           
        # Save Log  
        with open(result_filename, 'a') as file:
            file.write('Model = %s\n' % (model_names[i]))
            file.write('BAcc = %.4f\n' % (np.array(model_nested_score['bacc']).mean()))
            file.write('F1 = %.4f\n' % (np.array(model_nested_score['f1']).mean()))
            file.write('Recall = %.4f\n' % (np.array(model_nested_score['recall']).mean()))
            file.write('Precision = %.4f\n' % (np.array(model_nested_score['precision']).mean()))
            file.write('Mcc = %.4f\n' % (np.array(model_nested_score['mcc']).mean()))
            file.write('Kappa = %.4f\n' % (np.array(model_nested_score['kappa']).mean()))
            file.write('Best estimator:\n')
            file.write('\tFit time = %.3fs (+/- %.3fs)\n' % (np.array(model_nested_score['fit_time']).mean(), np.array(model_nested_score['fit_time']).std()))
            file.write('\tPredict time = %.3fs (+/- %.3fs)\n' % (np.array(model_nested_score['predict_time']).mean(), np.array(model_nested_score['predict_time']).std()))
            file.write('\tMemory size = %.3f MB (+/- %.3f MB)\n' % (bytesto(np.array(model_nested_score['memory']).mean(), 'm'), bytesto(np.array(model_nested_score['memory']).std(), 'm')))
            file.write('Best estimator parameters:\n')
            for j,best_params in enumerate(model_nested_score['best_params'],start=1):
                file.write('\tEstimator %d = %s\n' % (j, str(best_params)))
            file.write('Fit time:\n')
            for j,value in enumerate(model_nested_score['fold_time'], start=1):
                file.write('\tFold %d = %.3fs\n' % (j, value))
            file.write('Total Time = %.3fs\n' % (endModel - startModel))
            file.write('\n')
        
        
        # Print model finished
        print("%s finished. Elapsed %.3fs\n" % (model_names[i], endModel - startModel))
        
        # Save model results pickle
        model_filename = './Results/' + model_acronyms[i] + '.pkl'
        with open(model_filename,'wb') as file:
            pickle.dump(model_nested_score, file)
    
    
    # All models Nested CV ending time
    endFull = time.time()
    
    # Total time to results file
    with open(result_filename,'a') as file:
        file.write('Full CV Time = %.3fs\n' % (endFull - startFull))
    
    # Populate results pickle
    results['nested_score'] = nested_score
    results['total_time'] = (endFull - startFull)
    results['model_acronyms'] = model_acronyms
    results['model_param_grid'] = models_param_grid
    results['model_names'] = model_names
    
    # Remove models folder
    os.rmdir(model_folder)
    
    # Save results pickle
    result_pickle_filename  = './Results/Results.pkl'
    with open(result_pickle_filename, 'wb') as file:
        pickle.dump(results, file)
        

if __name__ == '__main__':
    # Load Dataset
    X = np.load('../Datasets/Ember/ember2018/X_20k.npy')
    y = np.load('../Datasets/Ember/ember2018/y_20k.npy')

    # Nested Grid Search Cross Validation
    crossValidation(X, y)
